{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "## Introduction:\n",
    "\n",
    "Uber announced that its users in New York City could order yellow taxis through the Uber app in the future. To explore the trends for Uber and yellow taxi, we make analysis based on hired-ride trip data from Uber and NYC Yellow cab from January 2009 through June 2015, and local historical weather data.\n",
    "##### The analysis is mainly broken up into 4 Parts: (Detailed analysis about each part are shown in the following Jupyter Notebook)\n",
    "<br>\n",
    "Data Preprocessing\n",
    "<br>\n",
    "Storing Data\n",
    "<br>\n",
    "Understanding Data\n",
    "<br>\n",
    "Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "All import statements needed for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "from scipy.stats import sem\n",
    "from keplergl import KeplerGl\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "Define a functin called \"calculate_distance\" that calculates the distance between two coordinates in kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_lat, from_long,to_lat,to_long):\n",
    "    \"\"\"Calculate the distance bewteen two coordinates in kilometers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    Inputs: \n",
    "        from_lat -- first coordinate's latitude\n",
    "        from_long -- first coordinate's longitude\n",
    "        to_lat -- second coordinate's latitude\n",
    "        to_long -- second coordinate's longitude\n",
    "    Output:\n",
    "        distance -- distance between two coordinates in kilometers\n",
    "    \"\"\"\n",
    "    \n",
    "    R = 6373.0\n",
    "    lat1 = radians(from_lat)\n",
    "    lon1 = radians(from_long)\n",
    "    lat2 = radians(to_lat)\n",
    "    lon2 = radians(to_long)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "For the taxi data, we first downloaded parquet files programmingly using regular expression,requests and beautifulsoup module. Then, we constructed a sampling of 3000 rows for each month data to make the sample size consistent with the one of Uber Data. The next step is cleaning data including removing invalid data, unnecessary columns, and add a column of distance according to the coordinates. Lastly, we append data of each month to a big dataframe.\n",
    "<br>\n",
    "\n",
    "##### Invalid Data Criteria:\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * passenger count=0\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * fare amount<=0\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * distance <=0\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * coordinates out of New York box or NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Find Urls of Taxi Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    \"\"\"Get Urls using requests and beautifulsoup\n",
    "    Keyword Arguments:\n",
    "    Output:\n",
    "        res -- A list contain all urls of yellow taxi montly data\n",
    "    \n",
    "    \"\"\"\n",
    "    TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    soup = bs4.BeautifulSoup(html,'html.parser')\n",
    "    w=soup.find_all(\"a\")\n",
    "    res=[]\n",
    "    for i in range(len(w)):\n",
    "        if w[i].text==\"Yellow Taxi Trip Records\":\n",
    "            res.append(w[i]['href'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Download Taxi Data from 2009-01 to 2015-06__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=find_taxi_csv_urls()\n",
    "for x in res:\n",
    "    #Use regular expression to extract required urls and use requests to download data\n",
    "    pattern = r\"(2009-\\d{2}|2010-\\d{2}|2011-\\d{2}|2012-\\d{2}|2013-\\d{2}|2014-\\d{2}|2015-0[1-6])\\.(parquet)\"\n",
    "    result = re.search(pattern, x)\n",
    "   \n",
    "    if result != None:\n",
    "        response = requests.get(x, stream=True)\n",
    "        title=result.groups()[0]\n",
    "    with open(title+\".parquet\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Clean Data from 2011 to 2015 (Location ID is provided instead of Coordinates)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = gpd.read_file('taxi_zones.shp')\n",
    "df2 = df2.to_crs(epsg=4326)  # EPSG 4326 = WGS84 = https://epsg.io/4326\n",
    "def find_long(ID,df2):\n",
    "    \"\"\"Find longitude using taxi_zones.shp corresponding to the Location ID\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        ID -- Location ID\n",
    "        df2 -- dataframe of taxi_zones.shp\n",
    "    \n",
    "    Outputs:\n",
    "        long-- longitude\n",
    "        np.nan-- if the coordinate is out of new york box\n",
    "    \"\"\"\n",
    "    long=df2.iloc[ID-1].geometry.centroid.x\n",
    "    if -74.242330<=long<=-73.717047:\n",
    "        return long\n",
    "    else:\n",
    "        return np.nan\n",
    "def find_lat(ID,df2):\n",
    "    \"\"\"Find latitude using taxi_zones.shp corresponding to the Location ID\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        ID -- Location ID\n",
    "        df2 -- dataframe of taxi_zones.shp \n",
    "    \n",
    "    Outputs:\n",
    "        lat-- latitude\n",
    "        np.nan-- if the coordinate is out of new york box\n",
    "    \"\"\"\n",
    "    lat=df2.iloc[ID-1].geometry.centroid.y\n",
    "    if 40.560445<=lat<=40.908524:\n",
    "        return lat\n",
    "    else:\n",
    "        return np.nan\n",
    "def normal_from_2011(parquet_file):\n",
    "    \"\"\"Clean Data: Normalize column names, remove invalid rows, sampling size=3000, find coordinates and add distance\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        parquet_file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance, and tip amount\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "    df = df[(df.passenger_count != 0) & (df.fare_amount > 0)]\n",
    "    df.rename(columns={'tpep_pickup_datetime':'pickup_datetime'},inplace=True)    \n",
    "    df.set_index(pd.to_datetime(df[\"pickup_datetime\"]),inplace=True)\n",
    "    df = df[[\"PULocationID\", \"DOLocationID\",\"tip_amount\"]]\n",
    "    taxi_df=df.sample(n=3000,random_state=100)\n",
    "    taxi_df = taxi_df.loc[(taxi_df['PULocationID'] < 264) & (taxi_df['PULocationID'] >= 1)]\n",
    "    taxi_df = taxi_df.loc[(taxi_df['DOLocationID'] < 264) & (taxi_df['DOLocationID'] >= 1)]\n",
    "    \n",
    "    taxi_df[\"pickup_latitude\"]=np.nan\n",
    "    taxi_df[\"pickup_longitude\"]=np.nan\n",
    "    taxi_df[\"dropoff_latitude\"]=np.nan\n",
    "    taxi_df[\"dropoff_longitude\"]=np.nan\n",
    "    lat1 = taxi_df.apply(\n",
    "        lambda row: find_lat(row[\"PULocationID\"].astype('int'),df2),axis=1)\n",
    "    long1 = taxi_df.apply(\n",
    "        lambda row: find_long(row[\"PULocationID\"].astype('int'),df2),axis=1)\n",
    "    \n",
    "    taxi_df[\"pickup_latitude\"] = lat1\n",
    "    taxi_df[\"pickup_longitude\"] = long1\n",
    " \n",
    "    lat2 = taxi_df.apply(\n",
    "        lambda row: find_lat(row[\"DOLocationID\"].astype('int'),df2),axis=1)\n",
    "    long2 = taxi_df.apply(\n",
    "    lambda row: find_long(row[\"DOLocationID\"].astype('int'),df2),axis=1)\n",
    "\n",
    "    \n",
    "    taxi_df[\"dropoff_latitude\"] = lat2\n",
    "    taxi_df[\"dropoff_longitude\"] = long2   \n",
    "    \n",
    "    add_distance = taxi_df.apply(\n",
    "                 lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),\n",
    "                 axis=1)\n",
    "    taxi_df['distance'] = add_distance\n",
    "    taxi_df = taxi_df[taxi_df.distance > 0]\n",
    "    taxi_df.dropna(inplace=True)\n",
    "    \n",
    "    return pd.DataFrame(taxi_df, columns=[\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\"dropoff_latitude\",\"tip_amount\",\"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Clean Data of 2009 and 2010 Respectively__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2009(file):\n",
    "    \"\"\"Clean Data: extract necessary columns and normalize names, remove invalid rows, sample size=3000\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        par -- dataframe with columns of datetime, coordinates of pickup&dropoff and tip amount\n",
    "    \"\"\"\n",
    "    par=pd.read_parquet(file,engine='pyarrow')\n",
    "    par.rename(columns={'Trip_Pickup_DateTime':'pickup_datetime'},inplace=True)\n",
    "    par.set_index(pd.to_datetime(par['pickup_datetime']),inplace=True)\n",
    "    par = par[(par.Passenger_Count != 0) & (par.Fare_Amt > 0)]\n",
    "    par=par[['Start_Lon','Start_Lat','End_Lon','End_Lat','Tip_Amt']]\n",
    "    par=par.sample(3000,random_state=100)\n",
    "    par.rename(columns={'Start_Lon': 'pickup_longitude', 'Start_Lat': 'pickup_latitude','End_Lon':'dropoff_longitude','End_Lat':'dropoff_latitude','Tip_Amt':'tip_amount'},inplace=True)\n",
    "    return par\n",
    "def norm_2010(parquet_file): \n",
    "    \"\"\"Clean Data: extract necessary columns and normalize names, remove invalid rows, sample size=3000\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        parquet_file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff and tip amount\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "    df.set_index(pd.to_datetime(df[\"pickup_datetime\"]),inplace=True)\n",
    "    df = df[(df.passenger_count != 0) & (df.fare_amount > 0)]\n",
    "    df = df[[\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\",\"tip_amount\"]]\n",
    "    taxi_df=df.sample(n=3000,random_state=100)\n",
    "    return taxi_df\n",
    "def normal_before_2011(file):\n",
    "    \"\"\"Calculate Distance: remove rows with coordinates outside of new york box and add distance\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- sampled dataframe \n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance and tip amount\n",
    "    \"\"\"\n",
    "    if file[:4]==\"2009\":\n",
    "        taxi_df=norm_2009(file)\n",
    "    else:\n",
    "        taxi_df=norm_2010(file)\n",
    "    taxi_df=taxi_df.loc[(taxi_df[\"pickup_latitude\"]<=40.908524)&(taxi_df[\"pickup_latitude\"]>=40.560445)&(taxi_df[\"dropoff_latitude\"]<=40.908524)&(taxi_df[\"dropoff_latitude\"]>=40.560445)&(taxi_df[\"pickup_longitude\"]<=-73.717047)&(taxi_df[\"pickup_longitude\"]>=-74.242330)&(taxi_df[\"dropoff_longitude\"]<=-73.717047)&(taxi_df[\"dropoff_longitude\"]>=-74.242330)].copy()\n",
    "    add_distance = taxi_df.apply(\n",
    "        lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),axis=1)\n",
    "    taxi_df['distance'] = add_distance\n",
    "    taxi_df = taxi_df[taxi_df.distance > 0]\n",
    "    taxi_df.dropna(inplace = True)\n",
    "    return taxi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list storing all the file names of yellow taxi monthly data\n",
    "res=find_taxi_csv_urls()\n",
    "title=[]\n",
    "for x in res:\n",
    "    pattern = r\"(2009-\\d{2}|2010-\\d{2}|2011-\\d{2}|2012-\\d{2}|2013-\\d{2}|2014-\\d{2}|2015-0[1-6])\\.(parquet)\"\n",
    "    result = re.search(pattern, x)\n",
    "   \n",
    "    if result != None:\n",
    "        title.append(result.groups()[0]+\".parquet\")\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Append Monthly Data to a Big Dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(title):\n",
    "    \"\"\"Process Monthly Data and append to a single DataFrame\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- sampled dataframe \n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance and tip amount\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []    \n",
    "    for urls in title:\n",
    "        if urls[:4] == \"2009\" or  urls[:4] == \"2010\":\n",
    "            all_taxi_dataframes.append(normal_before_2011(urls))\n",
    "        else:\n",
    "            all_taxi_dataframes.append(normal_from_2011(urls))\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Taxi_Data=get_and_clean_taxi_data(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "##### 1. Manually downloaded and stored Uber data as \"uber_rides_sample.csv\"\n",
    "\n",
    "##### 2. Replace index with pickup_datetime\n",
    "\n",
    "##### 3. Remove invalid trips\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * Trips outside the required coordinate box\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * Trips with zero passenger count\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * Trips with no fare\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * Trips with no distance between dropoff and pickup\n",
    "##### 4. Remove unnecessary columns\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The dataset now only has 4 columns which represent longtitudes and latitudes respectively. \n",
    "##### 5. Add distance column\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Implemented calculate_distance function and add distance as a new column\n",
    "##### 6. Drop NaN & Normalize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"Load and clean the Uber data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    Inputs: \n",
    "        csv_file -- Uber data's file name\n",
    "    Output:\n",
    "        uber -- cleaned dataframe with columns of pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, and distance\n",
    "    \"\"\"\n",
    "    uber = pd.read_csv(csv_file) \n",
    "    uber.set_index(pd.to_datetime(uber['pickup_datetime']),inplace=True)\n",
    "    uber.drop([\"key\",\"Unnamed: 0\",'pickup_datetime'],axis=1,inplace=True)\n",
    "    uber = uber[(uber.passenger_count != 0) & (uber.fare_amount > 0)]\n",
    "    uber.drop([\"passenger_count\",\"fare_amount\"],axis=1,inplace=True)\n",
    "    uber['pickup_latitude'].apply(lambda x: float(x))\n",
    "    uber['pickup_longitude'].apply(lambda x: float(x))\n",
    "    uber['dropoff_latitude'].apply(lambda x: float(x))\n",
    "    uber['dropoff_longitude'].apply(lambda x: float(x))\n",
    "    uber=uber.loc[(uber[\"pickup_latitude\"]<=40.908524)&(uber[\"pickup_latitude\"]>=40.560445)&(uber[\"dropoff_latitude\"]<=40.908524)&(uber[\"dropoff_latitude\"]>=40.560445)&(uber[\"pickup_longitude\"]<=-73.717047)&(uber[\"pickup_longitude\"]>=-74.242330)&(uber[\"dropoff_longitude\"]<=-73.717047)&(uber[\"dropoff_longitude\"]>=-74.242330)]\n",
    "    add_distance = uber.apply(\n",
    "        lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),axis=1)\n",
    "    uber['distance'] = add_distance\n",
    "    uber = uber[uber.distance > 0]\n",
    "    uber.dropna(inplace = True)\n",
    "    \n",
    "    return uber\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uber_Data=load_and_clean_uber_data(\"uber_rides_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "For Weather Data, we manually downloaded them and stored as csv files. Then, we created two functions to manipulate hourly and daily data respectively. Moreover, within each function, we cleaned data by removing invalid data, removing unnecessary data, and transforming to appropriate column data type. After weather data processing, we got 2 dataframes. One is called Hourly_Weather_Data with index of datetime, columns of HourlyPrecipitation and HourlyWindSpeed. Another is called Daily_Weather_Data with index of datetime, columns of DailyAverageWindSpeed and DailyPrecipitation.\n",
    "\n",
    "##### 1. Clean hourly weather data\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Extract only useful columns: \"DATE\",\"HourlyPrecipitation\",and \"HourlyWindSpeed\"\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Replace \"T\" with 0 since it indicates trace amount of precipitation\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Convert HourlyPrecipitation data into numeric type since there are some wrongly denoted data such as \"0.12s\"\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Convert DATE to datetime type and replace the index with it\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Group the data into hourly data using resample('60min').mean() in pandas\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Construct cleaned dataframe \"Hourly_Weather_Data\" with index of DATE, columns of HourlyPrecipitation and HourlyWindSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"Process and Clean Hourly Weather Data\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        csv_file -- string of the name of weather file\n",
    "    Outputs:\n",
    "        weather -- Dataframe with columns of datetime, hourly precipitaion, and hourly wind speed\n",
    "    \n",
    "    \"\"\"\n",
    "    weather = pd.read_csv(csv_file,low_memory=False)\n",
    "    weather = weather[[\"DATE\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    weather.replace(to_replace='T',value=0,inplace=True)\n",
    "    weather['HourlyPrecipitation']=pd.to_numeric(weather['HourlyPrecipitation'],errors='coerce')\n",
    "    weather.set_index(pd.to_datetime(weather[\"DATE\"]),inplace=True)\n",
    "    weather=weather.resample('60min').mean()\n",
    "    weather = weather[[\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Clean daily weather data\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  For data from 2009 to 2011, daily data is missing, so we calculate daily data from hourly data:\n",
    "<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Take the mean value for windspeed and take the sum for precipitation*\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Replace \"T\" with 0 since it indicates trace amount of precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\"Process and Clean Daily Weather Data\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        csv_file -- string of the name of weather file\n",
    "    Outputs:\n",
    "        weather -- Dataframe with columns of date, hourly precipitaion, and hourly wind speed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    weather=pd.read_csv(csv_file,low_memory=False)\n",
    "    weather=weather[[\"DATE\",\"DailyAverageWindSpeed\",\"DailyPrecipitation\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    weather.replace(to_replace='T',value=0,inplace=True)\n",
    "    if int(csv_file[:4])<2012:\n",
    "        weather=weather[[\"DATE\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "        weather.set_index(pd.to_datetime(weather['DATE']),inplace=True)\n",
    "        weather[\"HourlyPrecipitation\"]=pd.to_numeric(weather[\"HourlyPrecipitation\"])\n",
    "        precipitation=weather[[\"DATE\",\"HourlyPrecipitation\"]].copy()\n",
    "        precipitation.dropna(inplace=True)\n",
    "        wind=weather[[\"DATE\",\"HourlyWindSpeed\"]].copy()\n",
    "        wind.dropna(inplace=True)\n",
    "        b=precipitation.HourlyPrecipitation.resample('D').sum()\n",
    "        a=wind.HourlyWindSpeed.resample('D').mean()\n",
    "        weather=pd.concat([a,b],axis=1)\n",
    "        weather.rename(columns={'HourlyWindSpeed': 'DailyAverageWindSpeed', 'HourlyPrecipitation': 'DailyPrecipitation'}, inplace=True)\n",
    "    else:\n",
    "        weather.set_index(pd.to_datetime(weather['DATE']).dt.date,inplace=True)\n",
    "        weather.index = pd.to_datetime(weather.index)\n",
    "        weather[\"DailyPrecipitation\"]=pd.to_numeric(weather[\"DailyPrecipitation\"])\n",
    "        weather=weather[[\"DailyAverageWindSpeed\",\"DailyPrecipitation\"]]\n",
    "        weather.dropna(subset=[\"DailyAverageWindSpeed\",\"DailyPrecipitation\"],inplace=True)\n",
    "    return weather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Construct Hourly and Daily Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \"\"\"Process all weather data and produce two dataframes\n",
    "    Key Arguments:\n",
    "    Outputs:\n",
    "        hourly_data -- Dataframe with columns of datetime, hourly precipitaion, and hourly wind speed\n",
    "        daily_data -- Dataframe with columns of date, hourly precipitaion, and hourly wind speed    \n",
    "    \"\"\"\n",
    "\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files = [\"2009_weather.csv\",\"2010_weather.csv\",\"2011_weather.csv\",\"2012_weather.csv\",\"2013_weather.csv\",\"2014_weather.csv\",\"2015_weather.csv\"]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hourly_Weather_Data, Daily_Weather_Data = load_and_clean_weather_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
